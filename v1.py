import argparse, os #è§£æå‘½ä»¤è¡Œå‚æ•°
# import h5py
import numpy as np
import os
from PIL import Image
import openslide
from tqdm import tqdm
import matplotlib.pyplot as plt
from omegaconf import OmegaConf
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import pickle
import copy
import tifffile


from utils.hdf5 import get_sorted_h5_files, get_sorted_wsi_files, read_h5_coords
from utils.feature_hook import FeatureHook

from ldm.util import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler

# from create_patches_fp import WSIPatchExtractor
from patches_utils.create_patches_fp import WSIPatchExtractor

feat_maps = []


def load_model_from_config(config_path, ckpt_path, device="cuda", verbose=False):
    # verboseæ˜¯å¦æ‰“å°ç¼ºå¤±/å¤šä½™çš„ key
    print(f"[INFO] Loading model from: {ckpt_path}")
    config = OmegaConf.load(config_path)
    
    pl_sd = torch.load(ckpt_path, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"[INFO] Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]

    # æ‰“å°ç¼ºå¤±
    model = instantiate_from_config(config.model)
    missing_keys, unexpected_keys = model.load_state_dict(sd, strict=False)
    
    if verbose:
        if len(missing_keys) > 0:
            print("[WARNING] Missing keys:")
            print(missing_keys)
        if len(unexpected_keys) > 0:
            print("[WARNING] Unexpected keys:")
            print(unexpected_keys)

    model.to(device)
    model.eval()
    return model

# wsiåˆ’åˆ†
# å°†å…¨æ™¯å›¾åˆ’åˆ†ä¸ºå¤šä¸ªé‡å çš„å°çª—å£ï¼Œå…¨æ™¯å›¾çš„é«˜åº¦ï¼Œå…¨æ™¯å›¾çš„å®½åº¦ï¼Œæ¯ä¸ªå¤„ç†çª—å£çš„å¤§å°ï¼Œç›¸é‚»çª—å£çš„æ­¥é•¿
def get_views(panorama_height, panorama_width, window_size=64, stride=8):
    # æ½œåœ¨ç©ºé—´ä¸º 1/8
    panorama_height /= 8
    panorama_width /= 8
    # è®¡ç®—åœ¨ç»™å®šçš„å®½åº¦å’Œé«˜åº¦ä¸‹ï¼Œéœ€è¦å¤šå°‘ä¸ªçª—å£ï¼›æ»‘åŠ¨çª—å£çš„æ ‡å‡†è®¡ç®—æ–¹å¼
    num_blocks_height = (panorama_height - window_size) // stride + 1
    num_blocks_width = (panorama_width - window_size) // stride + 1
    total_num_blocks = int(num_blocks_height * num_blocks_width)

    # ç”Ÿæˆçª—å£åæ ‡
    # é€»è¾‘æ˜¯ä»å·¦åˆ°å³ä»ä¸Šåˆ°ä¸‹
    views = []
    for i in range(total_num_blocks): # i ï¼š 0 - n-1
        # h_end = h_start + window_size
        w_start = int((i % num_blocks_width) * stride)
        h_start = int((i // num_blocks_width) * stride)
        # w_end = w_start + window_size
        # views.append((h_start, h_end, w_start, w_end))
        views.append((w_start, h_start))  # åªéœ€è¦å·¦ä¸Šè§’åæ ‡
    return views

def get_opt():
    parser = argparse.ArgumentParser()
    # æ–‡ä»¶è·¯å¾„è®¾ç½®
    parser.add_argument('--wsi', help='Path to WSI file', required=True)
    parser.add_argument('--sty', help='Path to style image', required=True)
    parser.add_argument('--out', help='Output directory', required=True)
    parser.add_argument('--out_h5', help='Output directory', required=True)
    # åˆ‡ç‰‡å’Œhdf5è®¾ç½®
    parser.add_argument('--patch_size', type=int, default=512, help='Size of the patches')
    parser.add_argument('--stride', type=int, default=224, help='æ­¥é•¿')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for processing')
    parser.add_argument('--num_files', type=int, default=4, help='éœ€è¦å¤„ç†çš„æ–‡ä»¶æ•°é‡')
    parser.add_argument("--is_patch", action="store_true", help="æ˜¯å¦éœ€è¦è¿›è¡Œåˆ‡ç‰‡å¤„ç†")
    # æ¨¡å‹è®¾ç½®
    parser.add_argument('--model_config', type=str, default='models/ldm/stable-diffusion-v1/v1-inference.yaml', help='model coonfiguration æ–‡ä»¶')
    parser.add_argument('--ckpt', type=str, default='models/ldm/stable-diffusion-v1/model.ckpt', help='Path to the model checkpoint')
    parser.add_argument('--precision', type=str, default='autocast', help='choices: ["full", "autocast"]')
    # é£æ ¼æ³¨å…¥æ§åˆ¶
    parser.add_argument('--T', type=float, default=1.5, help='attention temperature scaling hyperparameter')
    parser.add_argument('--gamma', type=float, default=0.75, help='query preservation hyperparameter')
    parser.add_argument("--attn_layer", type=str, default='6,7,8,9,10,11', help='injection attention feature layers')
    # ddimè®¾ç½®
    parser.add_argument('--ddim_inv_steps', type=int, default=50, help='DDIM eta')
    parser.add_argument('--save_feat_steps', type=int, default=50, help='DDIM eta')
    parser.add_argument('--start_step', type=int, default=49, help='DDIM eta')
    parser.add_argument('--ddim_eta', type=float, default=0.0, help='DDIM eta')
    parser.add_argument('--C_latent', type=int, default=4, help='latent channels')
    # ä»£ç è¿è¡Œè®¾ç½®
    # parser.add_argument('--gpu', type=int, default=0, help='GPU ID to use')
    # åœ¨ get_opt() å‡½æ•°ä¸­
    parser.add_argument('--gpu', type=str, default='0', help='GPU IDs to use (e.g., "0" or "0,1")')
    # æ¨¡å—è®¾ç½®
    parser.add_argument("--without_init_adain", action='store_true')
    parser.add_argument("--without_attn_injection", action='store_true')

    opt = parser.parse_args()

    return opt


# é¢„å¤„ç†å›¾ç‰‡ å‰ªè£ å½’ä¸€åŒ–ï¼Œè°ƒæ•´é€šé“
def preprocess_img(path):
    image = Image.open(path).convert('RGB')
    x, y = image.size
    # print(f"ä» {path} åŠ è½½å›¾ç‰‡å¤§å°ä¸º ({x}, {y}) ")
    h = w = 512
    image = transforms.CenterCrop(min(x,y))(image) # (x, y)
    image = image.resize((w, h), resample=Image.Resampling.LANCZOS)
    image = np.array(image).astype(np.float32) / 255.0 # å°†å›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´ (512, 512, 3)
    image = image[None].transpose(0, 3, 1, 2) # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶è°ƒæ•´é€šé“é¡ºåº,å½¢çŠ¶ä» [H, W, 3] å˜ä¸º [1, H, W, 3]
    image = torch.from_numpy(image) # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è°ƒæ•´åƒç´ èŒƒå›´  (1, 3, 512, 512)
    return 2.*image - 1.

def preprocess_region(region: Image.Image):
    image = region  # å·²ç»æ˜¯ PIL.Image.convert("RGB")
    image = np.array(image).astype(np.float32) / 255.0
    image = image[None].transpose(0, 3, 1, 2)  # [1, 3, H, W]
    image = torch.from_numpy(image)
    return 2. * image - 1.  # æ˜ å°„åˆ° [-1, 1]



def extract_style_features(
    # --- é€šç”¨å‚æ•° ---
    model, 
    sampler, 
    uc,
    time_idx_dict, 
    save_feature_timesteps,
    # --- styleå‚æ•° ---
    img_dir,
    img_name, 
    feature_dir, 
    start_step=49, 
    save_feat=False,
    ddim_inversion_steps=50, 
    ddim_sampler_callback=None
    ):

    global feat_maps
    # feat_maps = []
    
    img_feature = None
    img_z_enc = None
    
    img_path = os.path.join(img_dir, img_name)
    print(f"[INFO] è¿›è¡ŒDDIMåæ¼”ï¼Œè·å–ç‰¹å¾å›¾: {img_path}")
    init_img = preprocess_img(img_path).to('cuda')
    img_feat_name = os.path.join(feature_dir, os.path.basename(img_name).split('.')[0] + '_sty.pkl')

    # 1. ç›´æ¥åŠ è½½ç‰¹å¾è¿”å› styleå›¾ç‰‡
    if os.path.exists(img_feat_name):
        print(f"[âœ“] åŠ è½½é£æ ¼ Loading style feature from {img_feat_name}")
        with open(img_feat_name, 'rb') as f:
            img_feature = pickle.load(f)
            img_z_enc = torch.clone(img_feature[0]['z_enc'])
        return img_feature, img_z_enc

    # 2. è¿›è¡Œddimåæ¼”ï¼Œè·å–ç‰¹å¾å›¾
    init_img = model.get_first_stage_encoding(model.encode_first_stage(init_img))  # [1, 4, 64, 64] z_0
    img_z_enc, _ = sampler.encode_ddim(init_img.clone(), num_steps = ddim_inversion_steps, \
                                        unconditional_conditioning = uc, \
                                        end_step = time_idx_dict[ddim_inversion_steps - 1 - start_step], \
                                        callback_ddim_timesteps = save_feature_timesteps, \
                                        img_callback = ddim_sampler_callback)
    print(f"DDIM Inversion Steps: {ddim_inversion_steps}")
    print(f"Start Step: {start_step}")
    print(f"Timesteps to save features at: {save_feature_timesteps}")
    print(f"End Step Index: {ddim_inversion_steps - 1 - start_step}")
    print(f"End Step Timestep: {time_idx_dict[ddim_inversion_steps - 1 - start_step]}")

    img_feature = copy.deepcopy(feat_maps)
    """ ==================== æ·»åŠ çš„è°ƒè¯•ä»£ç å¼€å§‹ ====================
    # å°† feat_maps çš„è¯¦ç»†ä¿¡æ¯ä¿å­˜åˆ° feat_maps_debug.txt æ–‡ä»¶ä¸­
    with open("feat_maps_debug_v1.txt", "w", encoding="utf-8") as f:
        f.write("--- Feat Maps Debug Info ---\n\n")
        f.write(f"Length of feat_maps: {len(feat_maps)}\n")
        f.write(f"Type of feat_maps: {type(feat_maps)}\n\n")

        if len(feat_maps) > 0:
            f.write(f"--- Details of the first element ---\n")
            f.write(f"Type of first element: {type(feat_maps[0])}\n")
            if isinstance(feat_maps[0], dict):
                f.write(f"Keys in first element: {list(feat_maps[0].keys())}\n")
        else:
            f.write("The feat_maps list is EMPTY.\n")
        
        f.write("\n--- Full content of feat_maps ---\n")
        f.write(str(feat_maps))

    print("[è°ƒè¯•ä¿¡æ¯] feat_maps çš„å†…å®¹å·²ä¿å­˜åˆ° feat_maps_debug_v1.txt æ–‡ä»¶ä¸­ï¼Œè¯·æŸ¥çœ‹ã€‚")
    # ==================== æ·»åŠ çš„è°ƒè¯•ä»£ç ç»“æŸ ==================== """

    # img_z_enc = feat_maps[0]['z_enc']
    img_z_enc = img_feature[0]['z_enc']


    if save_feat and len(feature_dir) > 0:
        print(f"ä¿å­˜é£æ ¼ style feature to {img_feat_name}")
        with open(img_feat_name, 'wb') as f:
            pickle.dump(img_feature, f)

    return img_feature, img_z_enc

def wsi_decode(
        latent_tensor: torch.Tensor,
        model: nn.Module,
        patch_size: int,
        stride: int,
        downsample_factor: int = 8,
        device: str = 'cuda',
        verbose: bool = True
    ) -> np.ndarray:
    """
    Args:
        latent_tensor (torch.Tensor): éœ€è¦è§£ç çš„å®Œæ•´latentå¼ é‡ï¼Œå½¢çŠ¶ä¸º (1, C, H_latent, W_latent)ã€‚
        model (nn.Module): åŒ…å« .decode_first_stage æ–¹æ³•çš„æ¨¡å‹å®ä¾‹ (å¦‚ Stable Diffusion çš„ VAE)ã€‚
        patch_size (int): åœ¨latentç©ºé—´ä¸­æ¯ä¸ªå›¾å—çš„å¤§å°ã€‚éœ€è¦å®éªŒæ‰¾åˆ°æ˜¾å­˜èƒ½æ‰¿å—çš„æœ€å¤§å€¼ä»¥æé«˜æ•ˆç‡ã€‚
        stride (int): åœ¨latentç©ºé—´ä¸­æ»‘åŠ¨çš„æ­¥é•¿ã€‚å¿…é¡»å°äº patch_size ä»¥å½¢æˆé‡å ã€‚
        downsampling_factor (int): VAEæ¨¡å‹çš„ä¸‹é‡‡æ ·å› å­ï¼Œé€šå¸¸æ˜¯ 8ã€‚
        device (str): ç”¨äºè§£ç çš„è®¾å¤‡ï¼Œä¾‹å¦‚ 'cuda' æˆ– 'cpu'ã€‚
        verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ã€‚
    Returns:
        np.ndarray: è§£ç å¹¶æ‹¼æ¥å®Œæˆçš„æœ€ç»ˆRGBå›¾åƒï¼Œå½¢çŠ¶ä¸º (H, W, 3)ï¼Œæ•°æ®ç±»å‹ä¸º uint8ã€‚
    
    if stride > patch_size:
        raise ValueError("Stride must be less than or equal to patch_size for overlapping.")

    with torch.no_grad():
        B, C, H_latent, W_latent = latent_tensor.shape

        H_pixel, W_pixel = H_latent * downsample_factor, W_latent * downsample_factor

        # åœ¨cpuå†…å­˜ä¸­åˆ›å»ºæœ€ç»ˆè¾“å‡ºå›¾åƒç”»å¸ƒ
        output_image = np.zeros((H_pixel, W_pixel, 3), dtype=np.uint8)

        latent_tensor = latent_tensor.to("cpu") # å°å—ä¼ è¾“åˆ°gpu

        # åˆ›å»ºè¿­ä»£å™¨å’Œè¿›åº¦æ¡
        y_steps = range(0, H_latent, stride)
        x_steps = range(0, W_latent, stride)

        if verbose:
            pbar = tqdm(total=len(y_steps) * len(x_steps), desc="Tiled Decoding")

        for y in y_steps:
            for x in x_steps:
                # 1. åˆ‡åˆ†å‡ºä¸€ä¸ªpatch
                y_end = min(y + patch_size, H_latent)
                x_end = min(x + patch_size, W_latent)
                latent_patch = latent_tensor[:, :, y:y_end, x:x_end]  # è·å–

                # 2. å°†å°å—é€åˆ°GPUå¹¶æ‰§è¡Œç¼–ç 
                latent_patch_gpu = latent_patch.to(device)
                decoded_patch_gpu = model.decode_first_stage(latent_patch_gpu)  # è§£ç 

                # 3. å°†ç»“æœè½¬æ¢æ ¼å¼å¹¶ç§»å›CPU
                decoded_patch_cpu = decoded_patch_gpu.squeeze(0)  # (C, H, W)
                decoded_patch_cpu = decoded_patch_cpu.permute(1, 2, 0) # (H, W, C)
                # decoded_patch_cpu = (decoded_patch_cpu.clamp(0, 1) * 255).to(torch.uint8).cpu().numpy() # ä¿®æ”¹ä¸ºä»¥ä¸‹
                # 1. æ­£ç¡®åœ°å°† [-1, 1] èŒƒå›´æ˜ å°„åˆ° [0, 1]
                decoded_patch_cpu = (decoded_patch_cpu + 1.0) / 2.0
                # 2. å†å°† [0, 1] èŒƒå›´æ˜ å°„åˆ° [0, 255] å¹¶è½¬æ¢ç±»å‹
                decoded_patch_cpu = (decoded_patch_cpu.clamp(0, 1) * 255).to(torch.uint8).cpu().numpy()

                # 4. è®¡ç®—éœ€è¦ç²˜è´´çš„åŒºåŸŸå’Œå°ºå¯¸
                # æˆ‘ä»¬åªç²˜è´´ç”±æ­¥é•¿ï¼ˆstrideï¼‰å†³å®šçš„â€œæ–°â€åŒºåŸŸï¼Œä»¥é¿å…é‡å åŒºåŸŸçš„ä¼ªå½±
                y_pixel_start = y * downsampling_factor
                x_pixel_start = x * downsampling_factor
                
                 # è®¡ç®—å®é™…è¦ä»è§£ç åpatchä¸­æå–çš„åŒºåŸŸå¤§å°
                h_paste_size = stride * downsampling_factor
                w_paste_size = stride * downsampling_factor

                # å¤„ç†å›¾åƒè¾¹ç•Œï¼Œé˜²æ­¢è¶Šç•Œ
                h_paste_size = min(h_paste_size, H_pixel - y_pixel_start)
                w_paste_size = min(w_paste_size, W_pixel - x_pixel_start)

                # 5. ä»è§£ç çš„patchä¸­å–å‡ºæœ‰æ•ˆéƒ¨åˆ†ï¼Œç²˜è´´åˆ°CPUå¤§ç”»å¸ƒä¸Š
                output_image[
                    y_pixel_start : y_pixel_start + h_paste_size,
                    x_pixel_start : x_pixel_start + w_paste_size,
                    :
                ] = decoded_patch_cpu[
                    :h_paste_size,
                    :w_paste_size,
                    :
                ]
                
                if verbose:
                    pbar.update(1)
        
        if verbose:
            pbar.close()

         # å¤„ç†é™¤ä»¥é›¶çš„æƒ…å†µ
        overlap_count[overlap_count == 0] = 1
        output_image /= overlap_count

        # è½¬æ¢å› uint8 å›¾åƒæ ¼å¼
        output_image = (output_image + 1.0) / 2.0 # ä» [-1, 1] è½¬æ¢åˆ° [0, 1]
        output_image = (output_image.clip(0, 1) * 255).astype(np.uint8)


    return output_image
    """
    if stride > patch_size:
        raise ValueError("Stride must be less than or equal to patch_size for overlapping.")

    with torch.no_grad():
        B, C, H_latent, W_latent = latent_tensor.shape
        H_pixel, W_pixel = H_latent * downsample_factor, W_latent * downsample_factor

        # åœ¨CPUå†…å­˜ä¸­åˆ›å»ºæœ€ç»ˆè¾“å‡ºå›¾åƒç”»å¸ƒ
        output_image = np.zeros((H_pixel, W_pixel, 3), dtype=np.uint8)

        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘åˆ é™¤ .to("cpu")ï¼Œè®©å¤§çš„latent_tensorä¿ç•™åœ¨GPUä¸Šç›´æ¥è¿›è¡Œåˆ‡ç‰‡ï¼Œæ•ˆç‡æ›´é«˜
        # latent_tensor = latent_tensor.to("cpu") 

        y_steps = range(0, H_latent, stride)
        x_steps = range(0, W_latent, stride)

        if verbose:
            pbar = tqdm(total=len(y_steps) * len(x_steps), desc="Tiled Decoding")

        for y in y_steps:
            for x in x_steps:
                y_end = min(y + patch_size, H_latent)
                x_end = min(x + patch_size, W_latent)
                
                # ç›´æ¥åœ¨GPUä¸Šè¿›è¡Œåˆ‡ç‰‡ï¼Œlatent_patchä»åœ¨GPUä¸Š
                latent_patch = latent_tensor[:, :, y:y_end, x:x_end]

                decoded_patch_gpu = model.decode_first_stage(latent_patch)

                decoded_patch_cpu = decoded_patch_gpu.squeeze(0).permute(1, 2, 0)

                # ã€Bug 1 å·²ä¿®å¤ã€‘æ­£ç¡®çš„é¢œè‰²å€¼è½¬æ¢æµç¨‹
                # 1. å…ˆå°† [-1, 1] èŒƒå›´æ˜ å°„åˆ° [0, 1]
                decoded_patch_cpu = (decoded_patch_cpu + 1.0) / 2.0
                # 2. å†å°† [0, 1] èŒƒå›´æ˜ å°„åˆ° [0, 255] å¹¶è½¬æ¢ç±»å‹
                decoded_patch_cpu = (decoded_patch_cpu.clamp(0, 1) * 255).to(torch.uint8).cpu().numpy()

                y_pixel_start = y * downsample_factor
                x_pixel_start = x * downsample_factor
                
                h_paste_size = stride * downsample_factor
                w_paste_size = stride * downsample_factor

                h_paste_size = min(h_paste_size, H_pixel - y_pixel_start)
                # ã€è¯­æ³•é”™è¯¯å·²ä¿®å¤ã€‘ç¡®ä¿æ­¤è¡Œä»£ç å®Œæ•´
                w_paste_size = min(w_paste_size, W_pixel - x_pixel_start)

                output_image[
                    y_pixel_start : y_pixel_start + h_paste_size,
                    x_pixel_start : x_pixel_start + w_paste_size,
                    :
                ] = decoded_patch_cpu[
                    :h_paste_size,
                    :w_paste_size,
                    :
                ]
                
                if verbose:
                    pbar.update(1)
        
        if verbose:
            pbar.close()

        # ã€Bug 2 å·²ä¿®å¤ã€‘åˆ é™¤äº†æ‰€æœ‰æ— æ•ˆçš„åå¤„ç†ä»£ç ï¼ˆå¦‚ overlap_count å’Œé‡å¤çš„é¢œè‰²è½¬æ¢ï¼‰

    return output_image


def main():
    opt = get_opt()

    device = "cuda" if torch.cuda.is_available() else "cpu"


    if opt.is_patch:
        wsi_extractor = WSIPatchExtractor()

        wsi_extractor.process(
            source=opt.wsi,
            save_dir=opt.out_h5,
            patch_size=opt.patch_size,
            step_size=opt.stride,
            patch_level=0,
            seg=True,
            patch=True,
            stitch=False,
            save_mask=False,
            auto_skip=True,
            num_files=opt.num_files
        )

    # wsi_extractor.extract_patches()

    # æ–‡ä»¶å¤¹å‡†å¤‡
    feature_dir = os.path.join(opt.out, 'features')
    os.makedirs(feature_dir, exist_ok=True)
    vstained_imgs_dir = os.path.join(opt.out, 'vstained_imgs')
    os.makedirs(vstained_imgs_dir, exist_ok=True)

    qkv_extraction_block_indices = list(map(int, opt.attn_layer.split(','))) # åœ¨unetçš„å“ªäº›blockå½“ä¸­æå–qkv
    ddim_inversion_steps = opt.ddim_inv_steps # ddimåæ¼”æ­¥æ•° 50 (encode_ddim)
    # save_feature_timesteps = ddim_steps = opt.save_feat_steps # ç‰¹å¾æå–/é£æ ¼æ³¨å…¥çš„æ—¶é—´æ­¥ = ä»noiseç”Ÿæˆå›¾åƒçš„æ­£å‘é‡‡æ ·æ­¥éª¤ï¼ˆsample_ddimï¼‰ 50
    ddim_steps = opt.save_feat_steps

    model = load_model_from_config(config_path=opt.model_config, ckpt_path=opt.ckpt, device='cuda')
    vae = model.first_stage_model
    unet = model.model.diffusion_model
    text_encoder = model.cond_stage_model

    # ddimsamplerå‡†å¤‡
    sampler = DDIMSampler(model)
    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=opt.ddim_eta, verbose=False) # ddim_stepså°±æ˜¯å¾ªç¯é‡‡æ ·çš„æ­¥æ•°
    # åœ¨è¿™ä¸ªåœ°æ–¹scheduleä¹‹åï¼Œsampler.ddim_timesteps å°±æ˜¯é‡‡æ ·çš„æ—¶é—´æ­¥å·²ç»ç¡®å®šäº†

    save_feature_timesteps = ddim_steps

    # è·å–ddimçš„æ—¶é—´æ­¥å’Œç´¢å¼•æ˜ å°„ å¹¶åè½¬ 
    time_range = np.flip(sampler.ddim_timesteps)
    """
    ddim_timesteps åœ¨ddim.pyä¸­ç”± make_ddim_timesteps() ç”Ÿæˆ
        åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼ŒDDIM å®é™…æ˜¯ä» t+1 åˆ° t åæ¨å›¾åƒï¼Œæ‰€ä»¥éœ€è¦å°†æ—¶é—´æ­¥æ•´ä½“å³ç§» 1
        ddim_timesteps = [0+1, 16+1, ..., 784+1] æ˜¯æ­£åºçš„
    åœ¨sampleçš„è¿‡ç¨‹ä¸­å®é™…ä¸Šæ˜¯è°ƒç”¨ ddim.pyä¸­çš„ddim_sampling() å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¸­è¿›è¡Œä»¥ä¸‹å¾ªç¯ï¼š
        timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps
        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)
        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps) # è¿›åº¦æ¡åŒ…è£…
        for i, step in enumerate(iterator):
    â€”â€” æ‰€ä»¥åœ¨ä¸»ç¨‹åºçš„é€»è¾‘å¾ªç¯ä¸­ï¼Œåº”è¯¥æ˜¯reverseçš„sampler.ddim_timestepï¼Œä¹Ÿå°±æ˜¯time_range
    idx time_dict = {981: 0, 961: 1, ... 1: 50} 1-50ç´¢å¼• 1-1000timestep
    time_idx_dict = {0: 981, 1: 961, ... 49: 21, 50: 1}
    """
    idx_time_dict = {} # å»å™ªæ—¶é—´æ­¥ï¼šddimé¡ºåºç´¢å¼•
    time_idx_dict = {} # ddimç´¢å¼•ï¼šå»å™ªæ—¶é—´æ­¥
    for i, t in enumerate(time_range):
        idx_time_dict[t] = i
        time_idx_dict[i] = t

    # åˆå§‹åŒ–å…¨å±€å˜é‡
    global feat_maps
    feat_maps = [{'config':{'gamma' : opt.gamma, 'T' : opt.T }} for _ in range(ddim_steps)]

    # åˆå§‹åŒ–ç‰¹å¾é’©å­
    feature_hook = FeatureHook(
        feat_maps=feat_maps,
        idx_time_dict=idx_time_dict,
        attn_layer_idics=qkv_extraction_block_indices,
        unet_model=unet
    )
    sty_ddim_sampler_callback = feature_hook.ddim_sampler_callback
    cnt_ddim_sampler_callback = feature_hook.content_q_update_callback

    # è·å–h5æ–‡ä»¶å’Œwsiæ–‡ä»¶
    h5_files = get_sorted_h5_files(opt.out_h5)
    wsi_files = get_sorted_wsi_files(opt.wsi)

    # æŸ“è‰²é£æ ¼å›¾ç‰‡ ç‰¹å¾è®¡ç®—å­˜å‚¨
    uc = model.get_learned_conditioning([""])   # è·å–æ¨¡å‹çš„æ— æ¡ä»¶å­¦ä¹ æ¡ä»¶ï¼Œä¹Ÿå°±æ˜¯è¾“å…¥æ–‡æœ¬

    # é£æ ¼å›¾ç‰‡ç‰¹å¾æå–
    sty_img_list = sorted(os.listdir(opt.sty))  # è·å–é£æ ¼å›¾ç‰‡åˆ—è¡¨ 
    # sty_feature, sty_z_enc = feature_extractor(opt.sty, sty_img, feature_dir, model, sampler, ddim_inversion_steps, uc, time_idx_dict, opt.start_step, save_feature_timesteps, ddim_sampler_callback, save_feat=True)
    
    for sty_img in sty_img_list:
        sty_feat, sty_z_enc = extract_style_features(img_dir=opt.sty, img_name=sty_img, feature_dir=feature_dir,save_feat=True,start_step=opt.start_step, model=model, sampler=sampler, uc=uc, time_idx_dict=time_idx_dict, ddim_sampler_callback=sty_ddim_sampler_callback, ddim_inversion_steps=ddim_inversion_steps, save_feature_timesteps=save_feature_timesteps)

    # éå†æ‰€æœ‰h5æ–‡ä»¶
    for idx, h5_file in tqdm(enumerate(h5_files), total=len(h5_files), desc="Processing H5 files"):
        # è¯»å–åæ ‡åˆ—è¡¨ æ‰“å¼€slide
        coords_list = read_h5_coords(h5_files[idx])
        slide = openslide.OpenSlide(wsi_files[idx])
        W, H = slide.dimensions
        
        patch_size = opt.patch_size
        patch_size_latent = opt.patch_size // 8 # åœ¨æ½œç©ºé—´å½“ä¸­éœ€è¦ç¼©æ”¾8å€

        # åˆ›å»ºå…¨æ™¯å›¾éœ€è¦çš„å¼ é‡
        latent = torch.zeros((1, opt.C_latent, H // 8, W // 8), device=device)
        count = torch.zeros_like(latent)
        value = torch.zeros_like(latent)
        # blank = torch.zeros_like(latent) # è®°å½•æœ‰å“ªäº›éƒ¨åˆ†æ²¡æœ‰è¢«å»å™ªï¼Œclamå·²ç»å»æ‰çš„éƒ¨åˆ†
        # v1.py, main() å‡½æ•°ä¸­
        blank = torch.zeros_like(latent, dtype=torch.bool)
        
        # æ„å»ºå¹¶ä¿å­˜åŸå§‹å›¾åƒçš„latent â€”â€” ç”¨äºèåˆ
        original_z0 = torch.zeros_like(latent)
        views = get_views(panorama_height=H, panorama_width=W,window_size=512, stride=512)

        # å¾ªç¯éå†coords_listï¼Œå¤„ç†èƒŒæ™¯ ä¸åº”è¯¥æ˜¯coord_list
        print(f"å¼€å§‹å¤„ç†èƒŒæ™¯")
        for coord in tqdm(views, desc="Building Original z0 background", unit="patch"):
            x_pixel, y_pixel = coord
            x_latent, y_latent = x_pixel // 8, y_pixel // 8

            region_img = slide.read_region((x_pixel, y_pixel), 0, (patch_size, patch_size)).convert("RGB")
            region_tensor = preprocess_region(region_img).to(device)  # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š

            # åªè¿›è¡ŒVAEç¼–ç ï¼Œä¸åŠ å™ª
            z_0_patch = model.get_first_stage_encoding(model.encode_first_stage(region_tensor))
        
            original_z0[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] += z_0_patch
            # print(f"å¤„ç†èƒŒæ™¯å®Œæˆï¼Œå½“å‰åæ ‡: {coord}, åŸå§‹z0å½¢çŠ¶: {original_z0.shape}")

        # original_z0 = torch.where(count_for_z0 > 0, original_z0 / count_for_z0, original_z0)
        # è‡³æ­¤ï¼Œoriginal_z0 å‡†å¤‡å®Œæ¯•ï¼Œå®ƒåŒ…å«äº†ç²¾ç¡®çš„åŸå§‹H&EèƒŒæ™¯latent

        # åˆå§‹latentçš„è·å–è¿‡ç¨‹ï¼Œddim reverse
        print(f"å¼€å§‹è·å–åˆå§‹latent")
        for coord in tqdm(coords_list, desc="Encoding patches", unit="patch"):
            x_pixel, y_pixel = coord
            x_latent = x_pixel // 8
            y_latent = y_pixel // 8

            region_img = slide.read_region((x_pixel, y_pixel), 0, (patch_size, patch_size)).convert("RGB")
            region_tensor = preprocess_region(region_img).to(device)  # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            z_0_patch = model.get_first_stage_encoding(model.encode_first_stage(region_tensor))  # shape: [1, C, h//8, w//8]
            # encode_ddimæ˜¯ä¸€ä¸ªâ€œåŠ å™ªâ€çš„è¿‡ç¨‹ï¼Œå®ç°çš„
            z_T_patch, _ = sampler.encode_ddim(
                    z_0_patch.clone(),
                    num_steps=ddim_inversion_steps,
                    unconditional_conditioning=uc,
                    end_step=time_idx_dict[ddim_inversion_steps - 1 - opt.start_step]
                )

            latent[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] += z_T_patch
            count[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] += 1
            blank[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] = True # è®°å½•æœ‰å“ªäº›éƒ¨åˆ†è¢«åŒ…å«åˆ°å»å™ªlistå½“ä¸­ 

        latent = torch.where(count > 0, latent / count, latent)  # é¿å…é™¤ä»¥0

        print(f"åˆå§‹latentè·å–å®Œæˆ")


        iterator = tqdm(time_range, desc='DDIM Sampler', total=ddim_inversion_steps)


        # å¼€å¯å¾ªç¯ éå†50æ­¥DDIMå»å™ª
        for i,  step in enumerate(iterator):    
            print(f"å¼€å§‹ç¬¬ {i+1} æ­¥å»å™ªï¼Œå½“å‰æ—¶é—´æ­¥: {step}")            
            count.zero_()
            value.zero_()

            # a. ä¸ºå½“å‰æ­¥éª¤å‡†å¤‡å‚æ•°
            index = ddim_inversion_steps - i - 1
            # .to(device) ç¡®ä¿ ts å¼ é‡å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
            ts = torch.full((1,), step, device=device, dtype=torch.long)
            
            # å¾ªç¯éå†æ¯ä¸ªçª—å£
            for coord in coords_list:
                x_pixel, y_pixel = coord # åœ¨æ½œç©ºé—´å½“ä¸­éœ€è¦ç¼©æ”¾8å€
                x_latent = x_pixel // 8
                y_latent = y_pixel // 8

                # è¿›è¡Œå¤„ç† åœ¨è¿™ä¸€æ­¥ä¸ºäº†ç®€åŒ–ï¼Œç›´æ¥å°è¯•é‡‡ç”¨æ¯ä¸€æ­¥æå–qkv
                # 1. æå–å½“å‰å†…å®¹å›¾çš„patch
                region_img = slide.read_region((x_pixel, y_pixel), 0, (patch_size, patch_size)).convert("RGB")
                region_tensor = preprocess_region(region_img).to(device)  # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                # a. VAEç¼–ç 
                z_0_patch = model.get_first_stage_encoding(model.encode_first_stage(region_tensor))  # shape: [1, C, h//8, w//8]
                # b. DDIM Inversion æ•è·ç‰¹å¾
                # _ = feature_extractor(purpose='content', model=model, sampler=sampler, uc=uc, time_idx_dict=time_idx_dict, ddim_sampler_callback=ddim_sampler_callback, direct_latent_input=z_0_patch, target_step_num=i)
                target_timestep_t = time_idx_dict[i]
                _, _ = sampler.encode_ddim(z_0_patch.clone(), 
                                            num_steps=ddim_inversion_steps,
                                            unconditional_conditioning=uc,
                                            end_step=i,
                                            # callback_ddim_timesteps=i,
                                            img_callback=cnt_ddim_sampler_callback)
                # æ­¤å¤„callback_ddim_timestepsä¸èƒ½ä¸º0ï¼Œè§£å†³ï¼š ä¸ä¼ å…¥callback_ddim_timestepsï¼Œåˆ™ encode__ddim() ä¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆnp.flip(self.ddim_timesteps)ï¼‰éƒ½è°ƒç”¨ img_callback
                # z_T_patch, _ = sampler.encode_ddim(
                #     z_0_patch.clone(),
                #     num_steps=ddim_inversion_steps,
                #     unconditional_conditioning=uc,
                #     end_step=time_idx_dict[ddim_inversion_steps - 1 - opt.start_step]
                # )
                # img_z_enc, _ = sampler.encode_ddim(init_img.clone(), 
                #                         num_steps = ddim_inversion_steps, \
                #                         unconditional_conditioning = uc, \
                #                         end_step = time_idx_dict[ddim_inversion_steps - 1 - start_step], \
                #                         callback_ddim_timesteps = save_feature_timesteps, \
                #                         img_callback = ddim_sampler_callback)
                injected_features_i = feat_maps[i] 


                # ä»å…¨å±€latentå½“ä¸­æå–å‡º patch view
                latent_patch = latent[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent]

                # 2. æ‰§è¡Œå•æ­¥å»å™ª
                latents_view_denoised, _ = sampler.p_sample_ddim(
                    x=latent_patch,
                    c=None,
                    t=ts , index=index, 
                    unconditional_conditioning=uc,
                    injected_features=injected_features_i,
                )

                value[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] += latents_view_denoised
                count[:, :, y_latent:y_latent+patch_size_latent, x_latent:x_latent+patch_size_latent] += 1

            # èåˆæ‰€æœ‰patches -- new latent
            latent = torch.where(count > 0, value / count, value)
            print(f"ç¬¬ {i+1} æ­¥å»å™ªå®Œæˆï¼Œå½“å‰æ—¶é—´æ­¥: {step}")
        print(f"æ‰€æœ‰å»å™ªæ­¥éª¤å®Œæˆ")
        
        # å¾ªç¯ç»“æŸä¹‹åï¼Œè§£ç  latent ç©ºé—´
        # a. ç™½è‰²èƒŒæ™¯latentå‘é‡å¡«å……blankä¸ºfalseçš„éƒ¨åˆ†
        final_combined_latent = torch.where(
            blank,                # æ¡ä»¶ï¼šå¦‚æœè¿™ä¸ªåŒºåŸŸè¢«å¤„ç†è¿‡ (True)
            latent,               # å°±ä½¿ç”¨å»å™ªç”Ÿæˆçš„ç»“æœ
            original_z0           # å¦åˆ™ (False)ï¼Œå°±ä½¿ç”¨åŸå§‹H&Eå›¾çš„latentï¼ˆå³èƒŒæ™¯latentï¼‰
        )

        print("Starting final tiled decoding...")
        final_wsi_np = wsi_decode(
            latent_tensor=final_combined_latent,
            model=model,
            patch_size=128,  # åœ¨latentç©ºé—´ï¼Œå¯¹åº”1024x1024åƒç´ ï¼Œå¯æ ¹æ®æ˜¾å­˜è°ƒæ•´
            stride=96,       # åœ¨latentç©ºé—´ï¼Œå¯¹åº”768x768åƒç´ çš„æ­¥é•¿
            device=device
        )
        
        # ä¸å¤Ÿå¤§ï¼Œä¿å­˜å›¾ç‰‡è¶…è¿‡äº†4gb
        # final_wsi_img = Image.fromarray(final_wsi_np) 
        # final_wsi_img.save(os.path.join(opt.out, f"final_decoded_{idx}.tiff"))
        print("ä½¿ç”¨ tifffile ä¿å­˜ä¸º BigTIFF æ ¼å¼...")
        output_path = os.path.join(opt.out, f"final_decoded_{idx}_{opt.ddim_inv_steps}.tiff")
        # ä½¿ç”¨ bigtiff=True æ¥ç¡®ä¿æ”¯æŒå¤§äº4GBçš„æ–‡ä»¶
        tifffile.imwrite(output_path, final_wsi_np, bigtiff=True)
        print(f"å›¾åƒå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")

        print("æ­£åœ¨ä¿å­˜PNGé¢„è§ˆå›¾...")
        # è®¡ç®—ç¼©å°åçš„å°ºå¯¸ï¼Œä¾‹å¦‚ï¼Œå®½åº¦ä¸º2048åƒç´ ï¼Œé«˜åº¦æŒ‰æ¯”ä¾‹ç¼©æ”¾
        preview_width = 4096
        width, height = final_wsi_img.size
        preview_height = int(height * (preview_width / width))
        preview_img = final_wsi_img.resize((preview_width, preview_height), Image.Resampling.LANCZOS)
        preview_img.save(os.path.join(opt.out, f"final_decoded_{idx}_preview.png"))
        print("PNGé¢„è§ˆå›¾ä¿å­˜å®Œæˆã€‚")


if __name__ == "__main__":
    main()


"""
# ç‰¹å¾æå– åœ¨ç‰¹å®šçš„æ—¶é—´æ­¥ save_feature_timesteps
def feature_extractor(
    # --- é€šç”¨å‚æ•° ---
    purpose='style', # æ¨¡å¼ ('style' æˆ– 'content')
    model, 
    sampler, 
    uc,
    time_idx_dict, 
    save_feature_timesteps,
    ddim_sampler_callback=None,
    ddim_inversion_steps=50, 
    # --- styleå‚æ•° ---
    img_dir,
    img_name, 
    feature_dir, 
    start_step=49, 
    save_feat=False
    # --- contentå‚æ•° ---
    direct_latent_input: torch.Tensor = None, # ç›´æ¥æ¥æ”¶content
    target_step_num=None,    # ğŸ‘ˆ æŒ‡å®šå•ä¸ªæ—¶é—´æ­¥ï¼ˆå¦‚30ï¼‰ï¼Œé»˜è®¤ä¸ºNoneä¸æå–
    ):

    global feat_maps
    feat_maps = []
    
    img_feature = None
    img_z_enc = None
    feature = None # æŒ‡å®šæ—¶é—´æ­¥çš„ç‰¹å¾

    if purpose == 'content':
        if direct_latent_input is None:
            raise ValueError("For 'content' purpose, provide the latent tensor via 'direct_latent_input'.")
        if target_step_num is None:
            raise ValueError("For 'content' purpose, specify the step number via 'target_step_num'.")
        if not (1 <= target_step_num <= ddim_inversion_steps):
            raise ValueError(f"'target_step_num' must be between 1 and {ddim_inversion_steps}.")

        # 1. è®¡ç®—å¾ªç¯ iå’Œ ç¡®åˆ‡çš„æ—¶é—´æ­¥
        target_timestep_t = time_idx_dict[target_step_index] # å®é™…æ—¶é—´æ­¥ t

        # 2. æ‰§è¡ŒDDIMåæ¼”
        _, _ = sampler.encode_ddim(direct_latent_input.clone(), num_steps=ddim_inversion_steps,
                                            unconditional_conditioning=uc,
                                            end_step=target_step_num,
                                            callback_ddim_timesteps=[target_timestep_t],
                                            img_callback=ddim_sampler_callback)
        
        return None    
    
    elif purpose == 'style':
        img_path = os.path.join(img_dir, img_name)
        init_img = preprocess_img(img_path).to('cuda')
        img_feat_name = os.path.join(feature_dir, os.path.basename(img_name).split('.')[0] + '_sty.pkl')

        # 1. ç›´æ¥åŠ è½½ç‰¹å¾è¿”å› styleå›¾ç‰‡
        if os.path.exists(img_feat_name):
            print(f"[âœ“] Loading style feature from {img_feat_name}")
            with open(img_feat_name, 'rb') as f:
                img_feature = pickle.load(f)
                img_z_enc = torch.clone(img_feature[0]['z_enc'])
            return img_feature, z_enc_startstep

        # 2. è¿›è¡Œddimåæ¼”ï¼Œè·å–ç‰¹å¾å›¾
        init_img = model.get_first_stage_encoding(model.encode_first_stage(init_img))  # [1, 4, 64, 64] z_0
        img_z_enc, _ = sampler.encode_ddim(init_img.clone(), num_steps = ddim_inversion_steps, \
                                            unconditional_conditioning = uc, \
                                            end_step = time_idx_dict[ddim_inversion_steps - 1 - start_step], \
                                            callback_ddim_timesteps = save_feature_timesteps, \
                                            img_callback = ddim_sampler_callback)
        
        img_feature = copy.deepcopy(feat_maps)
        # img_z_enc = feat_maps[0]['z_enc']
        img_z_enc = img_feature[0]['z_enc']


        if save_feat and len(feature_dir) > 0:
            print(f"Saving style feature to {img_feat_name}")
            with open(img_feat_name, 'wb') as f:
                pickle.dump(img_feature, f)

        return img_feature, img_z_enc
"""